{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Structuring E2E Applications",
   "id": "a56d264b24d017bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading OpenAI API Key",
   "id": "2443ebe82994578b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T10:38:46.029061Z",
     "start_time": "2025-07-27T10:38:46.024609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(dotenv_path='../../.env')  # Specify the path to your .env file\n",
    "\n",
    "# Access the environment variable\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the variable is loaded\n",
    "if api_key or api_key == \"\":\n",
    "    print(\"API key loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load API key.\")"
   ],
   "id": "1687d4cc23c58518",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initializing OpenAI API",
   "id": "a983dd262a2e0243"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T10:38:46.044910Z",
     "start_time": "2025-07-27T10:38:46.039559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ],
   "id": "c78451f0857525b7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Auxiliary Functions",
   "id": "5f03cac9c5ba5c83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Function to get the model response",
   "id": "86607d47058d2a05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T10:38:46.054908Z",
     "start_time": "2025-07-27T10:38:46.053014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Message:\n",
    "    def __init__(self, role, content):\n",
    "        self.role = role\n",
    "        self.content = content"
   ],
   "id": "bb23438284e02594",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T10:38:46.067994Z",
     "start_time": "2025-07-27T10:38:46.065739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "@retry(wait=(wait_random_exponential(min=5, max=40)), stop=stop_after_attempt(4))\n",
    "def get_response(req_msgs:list[Message], model=\"gpt-4o-mini\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Get a response from the OpenAI API.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input prompt for the model.\n",
    "    - model (str): The model to use.\n",
    "    - temperature (float): Sampling temperature. Default is 0.7.\n",
    "\n",
    "    Returns:\n",
    "    - str: The model's response.\n",
    "    \"\"\"\n",
    "    model_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[message.__dict__ for message in req_msgs],\n",
    "        temperature=temperature,\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    return model_response.choices[0].message.content"
   ],
   "id": "ef0ae60e6c6506da",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T10:38:46.078971Z",
     "start_time": "2025-07-27T10:38:46.075745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "class MessageLengthError(Exception):\n",
    "    \"\"\"Custom exception for message length errors.\"\"\"\n",
    "    def __init__(self, errors:[], message=\"Message length validation failed.\"):\n",
    "        \"\"\"\n",
    "        Initialize the MessageLengthError exception.\n",
    "        :param errors:\n",
    "        :param message:\n",
    "        \"\"\"\n",
    "        self.errors = errors\n",
    "        super().__init__(message)\n",
    "\n",
    "    def get_errors(self):\n",
    "        \"\"\"\n",
    "        Get the list of errors.\n",
    "        :return: List of errors.\n",
    "        \"\"\"\n",
    "        return self.errors\n",
    "\n",
    "def count_tokens(input_message: Message):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a message.\n",
    "\n",
    "    Parameters:\n",
    "    - input_message (Message): The message to count tokens for.\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of tokens in the message.\n",
    "    \"\"\"\n",
    "    my_encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    return len(my_encoding.encode(input_message.content))\n",
    "\n",
    "def validate_messages(msgs_to_validate: list[Message], max_total_tokens=4096, max_tokens_per_message=2048):\n",
    "    \"\"\"\n",
    "    Validate the messages to ensure they do not exceed the token limit.\n",
    "\n",
    "    Parameters:\n",
    "    - messages (list[Message]): The list of messages to validate.\n",
    "    - max_tokens (int): The maximum number of tokens allowed. Default is 4096.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    total_tokens = 0\n",
    "    for i, message in enumerate(msgs_to_validate):\n",
    "        msg_tokens = count_tokens(message)\n",
    "        total_tokens += msg_tokens\n",
    "        if msg_tokens > max_tokens_per_message:\n",
    "            errors.append(f\"Message {i+1} exceeds token limit: {msg_tokens} tokens (max {max_tokens_per_message} tokens)\")\n",
    "\n",
    "    if total_tokens > max_total_tokens:\n",
    "        errors.append(f\"Total tokens exceed limit: {total_tokens} tokens (max {max_total_tokens} tokens)\")\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        raise MessageLengthError(errors=errors, message=\"Message length validation failed.\")\n",
    "\n"
   ],
   "id": "399d5adcbf5cc091",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Examples",
   "id": "6ee5ffd3fc33801f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T10:38:49.721703Z",
     "start_time": "2025-07-27T10:38:46.085127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "books = \"The Great Gatsby, To Kill a Mockingbird, 1984, Pride and Prejudice, The Catcher in the Rye\"\n",
    "prompt = f\"For each book in {books} find the author and the year of publication, and return the results in a JSON format.\"\n",
    "messages = [Message(role=\"user\", content=prompt)]\n",
    "response = get_response(req_msgs=messages)\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(f\"\\nResponse:\\n{response}\")"
   ],
   "id": "4304c3399bd64b72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "For each book in The Great Gatsby, To Kill a Mockingbird, 1984, Pride and Prejudice, The Catcher in the Rye find the author and the year of publication, and return the results in a JSON format.\n",
      "\n",
      "Response:\n",
      "{\n",
      "  \"books\": [\n",
      "    {\n",
      "      \"title\": \"The Great Gatsby\",\n",
      "      \"author\": \"F. Scott Fitzgerald\",\n",
      "      \"year_of_publication\": 1925\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"To Kill a Mockingbird\",\n",
      "      \"author\": \"Harper Lee\",\n",
      "      \"year_of_publication\": 1960\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"1984\",\n",
      "      \"author\": \"George Orwell\",\n",
      "      \"year_of_publication\": 1949\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Pride and Prejudice\",\n",
      "      \"author\": \"Jane Austen\",\n",
      "      \"year_of_publication\": 1813\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"The Catcher in the Rye\",\n",
      "      \"author\": \"J.D. Salinger\",\n",
      "      \"year_of_publication\": 1951\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T10:38:49.750334Z",
     "start_time": "2025-07-27T10:38:49.746751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Validate the messages\n",
    "long_prompt = \"This is a very long prompt that exceeds the token limit. \" * 100\n",
    "long_messages = [Message(role=\"user\", content=long_prompt)]\n",
    "try:\n",
    "    validate_messages(long_messages, max_total_tokens=4096, max_tokens_per_message=1000)\n",
    "    print(f\"Messages are valid.\")\n",
    "except MessageLengthError as e:\n",
    "    errors = e.get_errors()\n",
    "    for error in errors:\n",
    "        print(f\"Error: {error}\")"
   ],
   "id": "9d1d87d5108b674e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Message 1 exceeds token limit: 1201 tokens (max 1000 tokens)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T10:38:49.760801Z",
     "start_time": "2025-07-27T10:38:49.759360Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ca9169c896f08589",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
